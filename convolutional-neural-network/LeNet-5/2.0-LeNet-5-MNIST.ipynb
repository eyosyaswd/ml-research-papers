{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: 2.0-LeNet-5-MNIST.ipynb\n",
    "# Author: Eyosyas Dagnachew\n",
    "# Description: Train LeNet-5 model on MNIST dataset. Remember, the purpose of this implementation is not to create the most optimal network to classift the MNIST dataset. \n",
    "#              Instead, it is to learn how to read a machine learning research paper and implement it (and in the process have a better understanding of the paper and its methodology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters/Hyperparameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 100        # 5000 (in the paper)\n",
    "NUM_EPOCHS = 20          # 2 (in the paper)\n",
    "LEARNING_RATE = 0.0005  # changing lr with iterations (in the paper)\n",
    "MOMENTUM = 0.02         # 0.02 (in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ../../data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ../../data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../../data\", \n",
    "                                           train=True, \n",
    "                                           transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                          download=False)\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    '''\n",
    "    Input: 32x32 pixel image in the paper, but 28x28 in the dataset\n",
    "           The paper mentions that \"[32x32] is significantly larger than the largest character in the\n",
    "           database (at most 20x20 pixels centered in a 28x28 field). This might explain why the \n",
    "           the images in this dataset have been cropped to 28x28. Using 2D convolution because the \n",
    "           input is technically a 3D (32x32x1) image.\n",
    "           \n",
    "           Activation Function: In the paper, the activation function used is the scaled hyperbolic\n",
    "                                tangent: f(a) = A*tanh(S*a), where A=1.71519 is the amplitude of the\n",
    "                                function and S determines its slope at the origin. This is a sigmoidal\n",
    "                                function. In my implmenetaion, I will initially just use the built-in\n",
    "                                nn.Sigmoid() actiation function, and evntually, I will implement the \n",
    "                                function used in the paper.\n",
    "           \n",
    "    Output: 10\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # Layer C1: conv layer with 6 28x28 feature maps with 5x5 kernels \n",
    "        #           parameters and connections: 156 trainable parameters, 122304 connections (28*28*156)\n",
    "        #           notes: padding=2 because the original images were 32x32 but this dataset contains 28x28 images (2 pixels removed \n",
    "        #                  from all sides), so we have to make up for the removed pixels but adding a padding of 2 on all sides\n",
    "        #           in: 32x32x1, out: 28x28x6\n",
    "        self.c1 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer S2: sub-sampling (pooling) layer with 6 14x14 feature maps with 2x2 kernels\n",
    "        #           parameters and connections: 12 trainable parameters, 5800 connections (in the paper);\n",
    "        #                                       0 trainable parameters, 5880 connections (in my implementation, read notes below)\n",
    "        #           notes: \"The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, \n",
    "        #                  and added to a trainable bias.\" This is where the difference between subsampling and pooling comes to play. \n",
    "        #                  Subsampling, as mentioned in the paper, is simply average pooling with learnable weights per feature map. \n",
    "        #                  In the Lua implementation of Torch, there is nn.SpatialSubSampling() but there is no such implementation\n",
    "        #                  for PyTorch, so I will just use average pooling, i.e. AvgPool2d().\n",
    "        #           in: 28x28x6, out: 14x14x6\n",
    "        self.s2 = nn.Sequential(\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),    # sub-sampling\n",
    "                    nn.Sigmoid()                              # sigmoidal function\n",
    "        )\n",
    "        \n",
    "        # Layer C3: conv layer with 16 10x10 feature maps with 5x5 kernels\n",
    "        #           parameters and connections: 1,516 trainable parameters and 151,600 connections (in the paper);\n",
    "        #                                       2,416 trainable parameters and 241,600 connections (in my implementation, read notes below)\n",
    "        #           notes: \"Each unit in each feature map is connected to several 5x5 neighborhoods at identical locations in a subset\n",
    "        #                  of S2's feature maps. Table I shows the set of S2 feature maps combined by each C3 feature maps.\" Instead of\n",
    "        #                  connecting each output channel with each input channel, they connect certain output channels with certain input \n",
    "        #                  channels (detailed in Table 1). This is done because of two reasons: (1) To reduce the number of connections, \n",
    "        #                  and (2) To force a break of symmetry in the network so that \"different feature maps are forced to extract \n",
    "        #                  different (hopefully complementary) features because they get different sets of input.\" I have no idea how to do\n",
    "        #                  this in PyTorch (or if there even is a way), so I'm just going to use the default (a.k.a. groups=1).\n",
    "        #           in: 14x14x6, out: 10x10x16\n",
    "        self.c3 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, groups=1),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        # Layer S4: sub-sampling (pooling) layer with 16 5x5 feature maps with 2x2 kernels\n",
    "        #           parameters and connections: 32 trainable parameters, 2,000 connections (in the paper)\n",
    "        #                                       0 trainable parameters, 2,000 connections (in my implementation, read Layer S2 notes)\n",
    "        #           notes: Connections between input and output are made in a similar way as C1 and S2.\n",
    "        #           in: 10x10x16, out: 5x5x16\n",
    "        self.s4 = nn.Sequential(\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer C5: conv layer with 120 1x1 feature maps with 5x5 kernels\n",
    "        #           parameters and connections: 48,120 trainable connections (in the paper)\n",
    "        #                                       48,120 trainable connections (in my implementation)\n",
    "        #           notes: Since the size of S4 is the same as the size of the kernels of C5 (5x5), the size of C5's feature maps is 1x1. \n",
    "        #                  This makes the connection between S4 and C5 a full connection. However, C5 is labelled as a conv layer instead\n",
    "        #                  of a fully-connected layer because \"if LeNet-5 were made bigger and everything else kept constant, the feature \n",
    "        #                  map dimensions would be larger than 1x1.\"\n",
    "        #           in: 5x5x16, out: 1x1x120\n",
    "        self.c5 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Layer F6: fully-connected layer with 84 units\n",
    "        #           parameters and connections: 10,164 trainable parameters (in the paper)\n",
    "        #                                       10,164 trainable parameters (in my implementation)\n",
    "        #           notes: why 84 units? explained in paper\n",
    "        #           in: 1x1x120, out: 1x1x84\n",
    "        self.f6 = nn.Sequential(\n",
    "                    nn.Linear(in_features=120, out_features=84),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Output Layer: Euclidean Radial Basis Function units (RBF), one for each class, with 84 inputs each\n",
    "        #               notes: \"Each output RBF unit computer the Euclidean distance between its input vector and its parameter vector. \n",
    "        #                      The further away is the input from the parameter vector, the larget is the RBF output. The output of a \n",
    "        #                      particular RBF can be interpreted as a penalty term measuring the fit between the input pattern and a model\n",
    "        #                      of the class associated with the RBF... Given an input pattern, the loss function should be designed so as \n",
    "        #                      to get the configuration of F6 as close as possible to the parameter vector of the RBF that corresponds to\n",
    "        #                      the pattern's desired class. The parameter vectors of these units were chose by hand and kept fixed...\"\n",
    "        #                      In the future, I will come back to this and try to have a better understanding of this, but for now I will \n",
    "        #                      just use another fully-connected layer with the built-in nn.Linear() function and a Softmax activation\n",
    "        #                      layer for the probabilities. dim=-1 picks the last dimension (NUM_CLASSES) as the one to compute Softmax on.\n",
    "        #               in: 1x1x84, out: 1x1xNUM_CLASSES\n",
    "        self.output = nn.Sequential(\n",
    "                        nn.Linear(in_features=84, out_features=NUM_CLASSES),\n",
    "                        nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):   # x.shape = torch.Size([100, 1, 28, 28])  (100 == batch_size)\n",
    "        out = self.c1(x)    # torch.Size([100, 6, 28, 28])\n",
    "        out = self.s2(out)  # torch.Size([100, 6, 14, 14])\n",
    "        out = self.c3(out)  # torch.Size([100, 16, 10, 10])\n",
    "        out = self.s4(out)  # torch.Size([100, 16, 5, 5])\n",
    "        out = self.c5(out)  # torch.Size([100, 120, 1, 1])\n",
    "        out = out.reshape(out.size(0), -1)  # torch.Size([100, 120])\n",
    "        out = self.f6(out)  # torch.Size([100, 84])\n",
    "        out = self.output(out)  # torch.Size([100, 10])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (c1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (s2): Sequential(\n",
      "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (c3): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (s4): Sequential(\n",
      "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (c5): Sequential(\n",
      "    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (f6): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "    (1): Softmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize LeNet model\n",
    "model = LeNet5(num_classes=NUM_CLASSES).to(device)\n",
    "print(model)\n",
    "\n",
    "# Print the number of parameters\n",
    "# for parameter in model.parameters():\n",
    "#     print(parameter.numel())    # 6 filters (150 each because 150/6 = 25 and 5x5 = 25 and weight sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer\n",
    "criterion = nn.MSELoss()    # requires one-hot encoding\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding buffer that I will reuse to store one-hot encoding of the labels\n",
    "labels_onehot = torch.FloatTensor(BATCH_SIZE, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/600], Loss: 0.0905\n",
      "Epoch [1/20], Step [200/600], Loss: 0.0907\n",
      "Epoch [1/20], Step [300/600], Loss: 0.0902\n",
      "Epoch [1/20], Step [400/600], Loss: 0.0910\n",
      "Epoch [1/20], Step [500/600], Loss: 0.0902\n",
      "Epoch [1/20], Step [600/600], Loss: 0.0907\n",
      "Epoch [2/20], Step [100/600], Loss: 0.0913\n",
      "Epoch [2/20], Step [200/600], Loss: 0.0921\n",
      "Epoch [2/20], Step [300/600], Loss: 0.0910\n",
      "Epoch [2/20], Step [400/600], Loss: 0.0905\n",
      "Epoch [2/20], Step [500/600], Loss: 0.0906\n",
      "Epoch [2/20], Step [600/600], Loss: 0.0897\n",
      "Epoch [3/20], Step [100/600], Loss: 0.0907\n",
      "Epoch [3/20], Step [200/600], Loss: 0.0908\n",
      "Epoch [3/20], Step [300/600], Loss: 0.0902\n",
      "Epoch [3/20], Step [400/600], Loss: 0.0903\n",
      "Epoch [3/20], Step [500/600], Loss: 0.0911\n",
      "Epoch [3/20], Step [600/600], Loss: 0.0907\n",
      "Epoch [4/20], Step [100/600], Loss: 0.0907\n",
      "Epoch [4/20], Step [200/600], Loss: 0.0902\n",
      "Epoch [4/20], Step [300/600], Loss: 0.0904\n",
      "Epoch [4/20], Step [400/600], Loss: 0.0907\n",
      "Epoch [4/20], Step [500/600], Loss: 0.0904\n",
      "Epoch [4/20], Step [600/600], Loss: 0.0907\n",
      "Epoch [5/20], Step [100/600], Loss: 0.0907\n",
      "Epoch [5/20], Step [200/600], Loss: 0.0908\n",
      "Epoch [5/20], Step [300/600], Loss: 0.0909\n",
      "Epoch [5/20], Step [400/600], Loss: 0.0904\n",
      "Epoch [5/20], Step [500/600], Loss: 0.0902\n",
      "Epoch [5/20], Step [600/600], Loss: 0.0909\n",
      "Epoch [6/20], Step [100/600], Loss: 0.0910\n",
      "Epoch [6/20], Step [200/600], Loss: 0.0906\n",
      "Epoch [6/20], Step [300/600], Loss: 0.0913\n",
      "Epoch [6/20], Step [400/600], Loss: 0.0912\n",
      "Epoch [6/20], Step [500/600], Loss: 0.0912\n",
      "Epoch [6/20], Step [600/600], Loss: 0.0904\n",
      "Epoch [7/20], Step [100/600], Loss: 0.0911\n",
      "Epoch [7/20], Step [200/600], Loss: 0.0902\n",
      "Epoch [7/20], Step [300/600], Loss: 0.0913\n",
      "Epoch [7/20], Step [400/600], Loss: 0.0912\n",
      "Epoch [7/20], Step [500/600], Loss: 0.0913\n",
      "Epoch [7/20], Step [600/600], Loss: 0.0905\n",
      "Epoch [8/20], Step [100/600], Loss: 0.0908\n",
      "Epoch [8/20], Step [200/600], Loss: 0.0904\n",
      "Epoch [8/20], Step [300/600], Loss: 0.0904\n",
      "Epoch [8/20], Step [400/600], Loss: 0.0906\n",
      "Epoch [8/20], Step [500/600], Loss: 0.0903\n",
      "Epoch [8/20], Step [600/600], Loss: 0.0906\n",
      "Epoch [9/20], Step [100/600], Loss: 0.0910\n",
      "Epoch [9/20], Step [200/600], Loss: 0.0901\n",
      "Epoch [9/20], Step [300/600], Loss: 0.0906\n",
      "Epoch [9/20], Step [400/600], Loss: 0.0915\n",
      "Epoch [9/20], Step [500/600], Loss: 0.0919\n",
      "Epoch [9/20], Step [600/600], Loss: 0.0906\n",
      "Epoch [10/20], Step [100/600], Loss: 0.0907\n",
      "Epoch [10/20], Step [200/600], Loss: 0.0909\n",
      "Epoch [10/20], Step [300/600], Loss: 0.0894\n",
      "Epoch [10/20], Step [400/600], Loss: 0.0901\n",
      "Epoch [10/20], Step [500/600], Loss: 0.0893\n",
      "Epoch [10/20], Step [600/600], Loss: 0.0906\n",
      "Epoch [11/20], Step [100/600], Loss: 0.0909\n",
      "Epoch [11/20], Step [200/600], Loss: 0.0909\n",
      "Epoch [11/20], Step [300/600], Loss: 0.0910\n",
      "Epoch [11/20], Step [400/600], Loss: 0.0908\n",
      "Epoch [11/20], Step [500/600], Loss: 0.0908\n",
      "Epoch [11/20], Step [600/600], Loss: 0.0907\n",
      "Epoch [12/20], Step [100/600], Loss: 0.0909\n",
      "Epoch [12/20], Step [200/600], Loss: 0.0911\n",
      "Epoch [12/20], Step [300/600], Loss: 0.0901\n",
      "Epoch [12/20], Step [400/600], Loss: 0.0902\n",
      "Epoch [12/20], Step [500/600], Loss: 0.0907\n",
      "Epoch [12/20], Step [600/600], Loss: 0.0906\n",
      "Epoch [13/20], Step [100/600], Loss: 0.0902\n",
      "Epoch [13/20], Step [200/600], Loss: 0.0909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-47a4b6f0af38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml-research-papers\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml-research-papers\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "NUM_ITERATIONS = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for iteration, training_batch in enumerate(train_loader):\n",
    "        # Split training batch data into images and labels\n",
    "        images, labels = training_batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Convert labels to one-hot encoding to pass to the MSE loss function\n",
    "        labels_onehot.zero_()\n",
    "        labels_onehot.scatter_(1, labels.reshape(-1,1), 1)\n",
    "        \n",
    "        # Forward pass and calculate loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels_onehot)\n",
    "        \n",
    "        # Zero the gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (iteration+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, NUM_EPOCHS, iteration+1, NUM_ITERATIONS, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "# torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
