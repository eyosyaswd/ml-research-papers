{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: 2.0-LeNet-5-MNIST.ipynb\n",
    "# Author: Eyosyas Dagnachew\n",
    "# Description: Train LeNet-5 model on MNIST dataset. Remember, the purpose of this implementation is not to create the most optimal network to classift the MNIST dataset. \n",
    "#              Instead, it is to learn how to read a machine learning research paper and implement it (and in the process have a better understanding of the paper and its methodology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters/Hyperparameters\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../../data\", \n",
    "                                           train=True, \n",
    "                                           transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                          download=False)\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    '''\n",
    "    Input: 32x32 pixel image in the paper, but 28x28 in the dataset\n",
    "           The paper mentions that \"[32x32] is significantly larger than the largest character in the\n",
    "           database (at most 20x20 pixels centered in a 28x28 field). This might explain why the \n",
    "           the images in this dataset have been cropped to 28x28.\n",
    "           \n",
    "           Using 2D convolution because the input is technically a 3D (32x32x1) image.\n",
    "           \n",
    "    Output: 10\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # Layer C1: conv layer with 6 28x28 feature maps with 5x5 kernels \n",
    "        #           parameters and connections: 156 trainable parameters, 122304 connections (28*28*156)\n",
    "        #           notes: padding=2 because the original images were 32x32 but this dataset contains 28x28 images (2 pixels removed \n",
    "        #                  from all sides), so we have to make up for the removed pixels but adding a padding of 2 on all sides\n",
    "        #           in: 32x32x1, out: 28x28x6\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2)    \n",
    "        \n",
    "        # Layer S2: sub-sampling (pooling) layer with 6 14x14 feature maps with 2x2 kernels; result are passed to sigmoidal function\n",
    "        #           parameters and connections: 12 trainable parameters, 5800 connections (in the paper);\n",
    "        #                                       0 trainable parameters, 5880 connections (in my implementation, read notes below)\n",
    "        #           notes: \"The four inputs to a unit in S2 are added, then multiplied by a trainable coefficient, \n",
    "        #                  and added to a trainable bias.\" This is where the difference between subsampling and pooling comes to play. \n",
    "        #                  Subsampling, as mentioned in the paper, is simply average pooling with learnable weights per feature map. \n",
    "        #                  In the Lua implementation of Torch, there is nn.SpatialSubSampling() but there is no such implementation\n",
    "        #                  for PyTorch, so I will just use average pooling, i.e. AvgPool2d().\n",
    "        #           in: 28x28x6, out: 14x14x6\n",
    "        self.s2 = nn.Sequential(\n",
    "                    nn.AvgPool2d(kernel_size=2, stride=2),    # sub-sampling\n",
    "                    nn.Sigmoid()                              # sigmoidal function\n",
    "        )\n",
    "        \n",
    "        # Layer C3: conv layer with 16 10x10 feature maps with 5x5 kernels\n",
    "        #           parameters and connections: 1,516 trainable parameters and 151,600 connections (in the paper);\n",
    "        #                                       2,416 trainable parameters and 241,600 connections (in my implementation, read notes below)\n",
    "        #           notes: \"Each unit in each feature map is connected to several 5x5 neighborhoods at identical locations in a subset\n",
    "        #                  of S2's feature maps. Table I shows the set of S2 feature maps combined by each C3 feature maps.\" Instead of\n",
    "        #                  connecting each output channel with each input channel, they connect certain output channels with certain input \n",
    "        #                  channels (detailed in Table 1). This is done because of two reasons: (1) To reduce the number of connections, \n",
    "        #                  and (2) To force a break of symmetry in the network so that \"different feature maps are forced to extract \n",
    "        #                  different (hopefully complementary) features because they get different sets of input.\" I have no idea how to do\n",
    "        #                  this in PyTorch (or if there even is a way), so I'm just going to use the default (a.k.a. groups=1).\n",
    "        #           in: 14x14x6, out: 10x10x16\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, groups=1)\n",
    "        \n",
    "        # Layer S4: sub-sampling (pooling) layer with 16 5x5 feature maps with 2x2 kernels\n",
    "        #           parameters and connections: 32 trainable parameters, 2,000 connections (in the paper)\n",
    "        #                                       0 trainable parameters, 2,000 connections (in my implementation, read Layer S2 notes)\n",
    "        #           notes: Connections between input and output are made in a similar way as C1 and S2.\n",
    "        #           in: 10x10x16, out: 5x5x16\n",
    "        self.s4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Layer C5: conv layer with 120 1x1 feature maps with 5x5 kernels\n",
    "        #           parameters and connections: 48,120 trainable connections (in the paper)\n",
    "        #                                       48,120 trainable connections (in my implementation)\n",
    "        #           notes: Since the size of S4 is the same as the size of the kernels of C5 (5x5), the size of C5's feature maps is 1x1. \n",
    "        #                  This makes the connection between S4 and C5 a full connection. However, C5 is labelled as a conv layer instead\n",
    "        #                  of a fully-connected layer because \"if LeNet-5 were made bigger and everything else kept constant, the feature \n",
    "        #                  map dimensions would be larger than 1x1.\"\n",
    "        #           in: 5x5x16, out: 1x1x120\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LeNet model\n",
    "model = LeNet5(num_classes=NUM_CLASSES).to(device)\n",
    "print(model)\n",
    "\n",
    "# Print the number of parameters\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.numel())    # 6 filters (150 each because 150/6 = 25 and 5x5 = 25 and weight sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
