{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: 1.0-initial-data-exploration.ipynb\n",
    "# Author: Eyosyas Dagnachew\n",
    "# Description: This file is just to explore the MNIST dataset and get to know it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"../../data\", \n",
    "                                           train=True, \n",
    "                                           transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                           download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"../../data\",\n",
    "                                          train=False,\n",
    "                                          transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                          download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x21442c8ab20>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x21442c8aac0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init data loader for MNIST dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)\n",
    "train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ../../data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ../../data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big is the dataset?\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(batch) = 2 --> batch[0] contains images and batch[1] contains labels.\n",
      "batch[0].size() = torch.Size([100, 1, 28, 28]) --> The images are 28x28 but PyTorch wants that 1 there for some reason.\n",
      "batch[1].size() = torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Understand the train_loader\n",
    "for batch in train_loader:    # iterates over batches\n",
    "    print(\"len(batch) = {} --> batch[0] contains images and batch[1] contains labels.\".format(len(batch)))\n",
    "    print(\"batch[0].size() = {} --> The images are 28x28 but PyTorch wants that 1 there for some reason.\".format(batch[0].size()))\n",
    "    print(\"batch[1].size() = {}\".format(batch[1].size()))\n",
    "    break\n",
    "    # batch is a tensor with size 2, one tensor with the images and one with the labels\n",
    "    # batch[0] has size 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a sample\n",
    "plt.imshow(batch[0][0].view(28,28)) # reshapes batch[0][0] from [1, 28, 28] to [28, 28], can also use plt.imshow(batch[0][0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      "0: 5923 images, 9.87%\n",
      "1: 6742 images, 11.24%\n",
      "2: 5958 images, 9.93%\n",
      "3: 6131 images, 10.22%\n",
      "4: 5842 images, 9.74%\n",
      "5: 5421 images, 9.04%\n",
      "6: 5918 images, 9.86%\n",
      "7: 6265 images, 10.44%\n",
      "8: 5851 images, 9.75%\n",
      "9: 5949 images, 9.92%\n",
      "Total Number of Training Images: 60000\n",
      "\n",
      "Testing Dataset\n",
      "0:  980 images, 9.80%\n",
      "1: 1135 images, 11.35%\n",
      "2: 1032 images, 10.32%\n",
      "3: 1010 images, 10.10%\n",
      "4:  982 images, 9.82%\n",
      "5:  892 images, 8.92%\n",
      "6:  958 images, 9.58%\n",
      "7: 1028 images, 10.28%\n",
      "8:  974 images, 9.74%\n",
      "9: 1009 images, 10.09%\n",
      "Total Number of Testing Images: 10000\n"
     ]
    }
   ],
   "source": [
    "# Are the classes balanced?\n",
    "total_train_images = 0\n",
    "train_class_cnt = Counter()\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    for label in y:\n",
    "        train_class_cnt[int(label)] += 1\n",
    "        total_train_images += 1\n",
    "\n",
    "total_test_images = 0\n",
    "test_class_cnt = Counter()\n",
    "for batch in test_loader:\n",
    "    X, y = batch\n",
    "    for label in y:\n",
    "        test_class_cnt[int(label)] += 1\n",
    "        total_test_images += 1\n",
    "\n",
    "print(\"Training Dataset\")\n",
    "train_class_cnt = OrderedDict(sorted(train_class_cnt.items(), key=lambda x: x[0]))\n",
    "for key, value in train_class_cnt.items():\n",
    "    print(\"{}: {:4} images, {:.2f}%\".format(key, value, (value/total_train_images)*100))\n",
    "print(\"Total Number of Training Images: {}\".format(total_train_images))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Testing Dataset\")\n",
    "test_class_cnt = OrderedDict(sorted(test_class_cnt.items(), key=lambda x: x[0]))\n",
    "for key, value in test_class_cnt.items():\n",
    "    print(\"{}: {:4} images, {:.2f}%\".format(key, value, (value/total_test_images)*100))\n",
    "print(\"Total Number of Testing Images: {}\".format(total_test_images))\n",
    "\n",
    "# Yes, the classes are balanced for the most part (~10% of each class in both training and testing datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
